# -*- coding: utf-8 -*-
"""Teste Técnico Engenheiro de Dados.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZWgLaAbAykc0bs4N-2GyDIlZiHinC2QY

# 1: Data Quality - Relatório de Falhas
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, sum as spark_sum, count, avg, percentile_approx,
    when, isnan, isnull, round as spark_round, lit, abs,
    row_number, desc, asc, sum
)
from pyspark.sql.types import StructType, StructField, LongType, StringType, DecimalType
from pyspark.sql.window import Window
import matplotlib.pyplot as plt
import time


# Configuração da Sessão Spark com otimizações de performance
spark_builder = SparkSession.builder \
    .appName("AnalisePedidosClientes") \
    .master("local[*]")

spark_builder = spark_builder.config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .config("spark.kryo.registrationRequired", "false") \
    .config("spark.sql.shuffle.partitions", "500") \
    .config("spark.default.parallelism", "200")

spark = spark_builder.getOrCreate()
print("Bibliotecas importadas.")
print("Sessão Spark criada com sucesso.")

# Esquema para Clientes
schema_clients = StructType([
    StructField("id", LongType(), True),
    StructField("name", StringType(), True)
])

path_clients = "clients/data.json"
df_clients = spark.read.json(path_clients, schema=schema_clients, multiLine=False)
df_clients = df_clients.repartition(4).cache()
print(f"Clientes carregados: {df_clients.count()}")
df_clients.show()

# Esquema para Pedidos
schema_pedidos = StructType([
    StructField("id", LongType(), True),
    StructField("client_id", LongType(), True),
    StructField("value", DecimalType(5, 2), True)
])

path_pedidos = "pedidos/data.json"
df_pedidos = spark.read.json(path_pedidos, schema=schema_pedidos, multiLine=False)
df_pedidos = df_pedidos.repartition(8).cache()
print(f"Pedidos carregados: {df_pedidos.count()}")
df_pedidos.show()

# Função de benchmark
def benchmark_configs(shuffle_partitions, default_parallelism):
    spark.conf.set("spark.sql.shuffle.partitions", str(shuffle_partitions))
    spark.conf.set("spark.default.parallelism", str(default_parallelism))

    print(f"\nTestando shuffle={shuffle_partitions}, parallelism={default_parallelism}...")
    start = time.time()

    df_result = df_pedidos.groupBy("client_id").agg(sum("value").alias("valor_total"))
    df_result.count()  # força execução

    end = time.time()
    tempo = end - start
    print(f"Tempo: {tempo:.2f} segundos")
    return tempo

# Valores para testar
shuffle_values = [100, 200, 500]
parallelism_values = [100, 200, 500]

resultados = {}
for s in shuffle_values:
    for p in parallelism_values:
        resultados[(s, p)] = benchmark_configs(s, p)

# Melhor combinação
melhor_config = min(resultados, key=resultados.get)
print("\n### Resultado Final ###")
print(f"Melhor configuração: shuffle={melhor_config[0]}, parallelism={melhor_config[1]}")

# 1.6 Validação de Clientes: Nulos ou NaN
df_clients_nulos_id = df_clients.filter(isnull(col("id"))) \
    .withColumn("campo", lit("id")).withColumn("motivo", lit("ID nulo (Clientes)")) \
    .select("id", "campo", "motivo")

df_clients_nulos_name = df_clients.filter(isnull(col("name"))) \
    .withColumn("campo", lit("name")).withColumn("motivo", lit("Nome nulo (Clientes)")) \
    .select("id", "campo", "motivo")

df_clients_falhas_nulos = df_clients_nulos_id.union(df_clients_nulos_name).orderBy(asc("id"))
print(f"Falhas por nulos/NaN em clientes: {df_clients_falhas_nulos.count()}")
df_clients_falhas_nulos.show()

# 1.7 Validação de Clientes: IDs duplicados
df_clients_ids_dup = (
    df_clients.groupBy("id")
    .agg(count("*").alias("qtd"))
    .filter(col("qtd") > 1)
    .orderBy(asc("id"))
)

print(f"Quantidade de IDs distintos duplicados em clientes: {df_clients_ids_dup.count()}")

distinct_df_clients_falhas_dup = (
    df_clients.join(df_clients_ids_dup.select("id"), on="id", how="inner")
    .withColumn("campo", lit("id"))
    .withColumn("motivo", lit("ID duplicado (Clientes)"))
    .select("id", "campo", "motivo")
    .distinct()
    .orderBy(asc("id"))
)
print(f"Quantidade total de IDs duplicados em clientes (após distinct): {distinct_df_clients_falhas_dup.count()}")
distinct_df_clients_falhas_dup.show()

# 1.7.1 Validação de Clientes: Nomes duplicados
df_clients_names_dup = (
    df_clients.groupBy("name")
    .agg(count("*").alias("qtd"))
    .filter(col("qtd") > 1)
    .orderBy(asc("name"))
)

print(f"Quantidade de nomes distintos duplicados em clientes: {df_clients_names_dup.count()}")
df_clients_names_dup.show()

# DataFrame com os nomes duplicados e seus IDs correspondentes
distinct_df_clients_names_dup = (
    df_clients.join(df_clients_names_dup.select("name"), on="name", how="inner")
    .withColumn("campo", lit("name"))
    .withColumn("motivo", lit("Nome duplicado (Clientes)"))
    .select("id", "name", "campo", "motivo")
    .orderBy(asc("name"))
)

print(f"\nQuantidade total de registros com nomes duplicados em clientes: {distinct_df_clients_names_dup.count()}")
distinct_df_clients_names_dup.show()


# 1.8 Relatório Consolidado de Falhas de Clientes
df_clients_relatorio_falhas = (
    df_clients_falhas_nulos.union(distinct_df_clients_falhas_dup)
    .cache()
)

total_clients_falhas = df_clients_relatorio_falhas.count()
print(f"\nTotal de falhas em clientes: {total_clients_falhas}")
df_clients_relatorio_falhas.show()

# 1.1 Falhas: Nulos ou NaN
df_nulos_id = df_pedidos.filter(isnull(col("id"))) \
    .withColumn("campo", lit("id")).withColumn("motivo", lit("ID nulo")) \
    .select("id", "campo", "motivo")

df_nulos_client_id = df_pedidos.filter(isnull(col("client_id"))) \
    .withColumn("campo", lit("client_id")).withColumn("motivo", lit("Client_ID nulo")) \
    .select("id", "campo", "motivo")

df_nulos_value = df_pedidos.filter(isnull(col("value")) | isnan(col("value"))) \
    .withColumn("campo", lit("value")).withColumn("motivo", lit("Value nulo/NaN")) \
    .select("id", "campo", "motivo")

df_falhas_nulos = df_nulos_id.union(df_nulos_client_id).union(df_nulos_value).orderBy(asc("id"))
print(f"Falhas por nulos/NaN: {df_falhas_nulos.count()}")
df_falhas_nulos.show()

# IDs duplicados (quantidade distinta)
df_ids_dup = (
    df_pedidos.groupBy("id")
    .agg(count("*").alias("qtd"))
    .filter(col("qtd") > 1)
    .orderBy(asc("id")) # ordena por id crescente
)

# Quantidade de IDs distintos duplicados
qtd_ids_dup = df_ids_dup.count()
print(f"Quantidade de IDs distintos duplicados de pedidos: {qtd_ids_dup}")

# DataFrame com todas as linhas duplicadas (mantendo colunas originais)
df_falhas_dup = (
    df_pedidos.join(df_ids_dup.select("id"), on="id", how="inner")  # pega todas as linhas desses IDs
    .withColumn("campo", lit("id"))
    .withColumn("motivo", lit("ID duplicado"))
    .select("id", "campo", "motivo")
)
print(f"Quantidade total de IDs duplicados em pedidos (incluindo a primeira ocorrência): {df_falhas_dup.count()}\n\n")

df_ids_dup.show()

distinct_df_falhas_dup = df_falhas_dup.select("id", "campo", "motivo").distinct().orderBy(asc("id")) # ordena por id crescente
distinct_df_falhas_dup.show()

# 1.3 Falhas: FK Violação (Client_ID não encontrado)
df_falhas_fk = df_pedidos.join(df_clients, df_pedidos.client_id == df_clients.id, "left_anti") \
    .withColumn("campo", lit("client_id")) \
    .withColumn("motivo", lit("Client_ID não encontrado em clientes")) \
    .select("id", "campo", "motivo")
print(f"FK violações: {df_falhas_fk.count()}")

# 1.4 Falhas: Valor Inválido (<0)
df_falhas_value = df_pedidos.filter(col("value") < 0) \
    .withColumn("campo", lit("value")) \
    .withColumn("motivo", lit("Value inválido (<0)")) \
    .select("id", "campo", "motivo")

quantidade_distinta_ids_valor_negativo = df_falhas_value.select("id").distinct().count()
print(f"Quantidade total de registros com o campo value negativo/inválido: {df_falhas_value.count()}")
print(f"Quantidade de IDs distintos dos values negativos/inválidos: {quantidade_distinta_ids_valor_negativo}")

# 1.5 Relatório Consolidado
df_relatorio_falhas = (
    df_falhas_nulos.union(distinct_df_falhas_dup)
    .union(df_falhas_fk)
    .union(df_falhas_value)
    .cache()
)

total_falhas = df_relatorio_falhas.count()
print(f"Total falhas: {total_falhas}")

print(f"\n\nPrimeiras falhas (ordenadas por ID crescente)")
df_relatorio_falhas.orderBy(asc("id")).show()

print(f"\n\nÚltimas falhas (ordenadas por ID decrescente)")
df_relatorio_falhas.orderBy(desc("id")).show()

print("Gerando gráfico de distribuição de falhas...")

# Agrupar por motivo e contar as ocorrências
motivos_counts = df_relatorio_falhas.groupBy("motivo").agg(count("*").alias("quantidade"))

# Coletar os dados para plotagem
data_for_plot = motivos_counts.collect()

labels = [row["motivo"] for row in data_for_plot]
sizes = [row["quantidade"] for row in data_for_plot]

# Criar o gráfico de pizza
plt.figure(figsize=(10, 7))
plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, pctdistance=0.85)

# Adicionar um círculo branco no centro para fazer um gráfico de rosca
centre_circle = plt.Circle((0,0),0.70,fc='white')
fig = plt.gcf()
fig.gca().add_artist(centre_circle)

plt.title("Distribuição Percentual das Falhas por Motivo")
plt.axis('equal') # Garante que o gráfico de pizza seja um círculo.
plt.show()

distinct_error_ids = df_relatorio_falhas.select("id").distinct()
print(f"Total de IDs de pedidos distintos com falhas: {distinct_error_ids.count()}")
distinct_error_ids.show()

total_orders = df_pedidos.count()
impacted_orders = distinct_error_ids.count()
impact_percentage = (impacted_orders / total_orders) * 100

print(f"Total de pedidos: {total_orders}")
print(f"Pedidos impactados por falhas: {impacted_orders}")
print(f"Percentual de impacto: {impact_percentage:.2f}%")

"""# 2: Agregação de Dados"""

df_pedidos_validos = df_pedidos.join(distinct_error_ids, df_pedidos.id == distinct_error_ids.id, "left_anti").orderBy(asc("client_id"))

print(f"Total de pedidos válidos: {df_pedidos_validos.count()}")
df_pedidos_validos.show()

# 1. Join especificando as tabelas e usando apelidos (aliases) para clareza
df_analise_clientes = df_pedidos_validos.alias("pedidos").join(
    df_clients.alias("clientes"),
    col("pedidos.client_id") == col("clientes.id"),
    "inner"
)

# 2. Agregação por Cliente
df_agregado = df_analise_clientes.groupBy(
    col("clientes.id"),
    col("clientes.name")
).agg(
    count(col("pedidos.id")).alias("quantidade_pedidos"),
    spark_sum(col("pedidos.value")).alias("valor_total_bruto")
)

# 3. Formatação Final e Ordenação
df_resultado_final = df_agregado.select(
    col("clientes.id").alias("id_cliente"),
    col("name").alias("nome_cliente"),
    col("quantidade_pedidos"),
    col("valor_total_bruto").cast("decimal(11,2)").alias("valor_total")
).orderBy(desc("valor_total"))

df_resultado_final.show()

# Soma total da coluna quantidade_pedidos
total_pedidos = df_resultado_final.agg(
    spark_sum("quantidade_pedidos").alias("total_pedidos")
).collect()[0]["total_pedidos"]

print(f"Total de pedidos realizados (soma de quantidade_pedidos): {total_pedidos}")

"""# 3: Análise Estatística"""

# Média aritmética
media_valor_total = df_resultado_final.agg(
    avg(col("valor_total")).alias("media")
).collect()[0]["media"]

# Mediana (percentil 50)
mediana_valor_total = df_resultado_final.agg(
    percentile_approx(col("valor_total"), 0.5).alias("mediana")
).collect()[0]["mediana"]

# Percentil 10 (10% inferiores)
percentil_10 = df_resultado_final.agg(
    percentile_approx(col("valor_total"), 0.1).alias("percentil_10")
).collect()[0]["percentil_10"]

# Percentil 90 (10% superiores)
percentil_90 = df_resultado_final.agg(
    percentile_approx(col("valor_total"), 0.9).alias("percentil_90")
).collect()[0]["percentil_90"]

# Exibição dos resultados
print("### 3. Análise Estatística")
print(f"Média aritmética: {media_valor_total}")
print(f"Mediana: {mediana_valor_total}")
print(f"Percentil 10 (10% inferiores): {percentil_10}")
print(f"Percentil 90 (10% superiores): {percentil_90}")

"""# 4: Filtragem - Acima da Média"""

df_acima_media = df_resultado_final.filter(
    col("valor_total") > media_valor_total
).orderBy(desc("valor_total"))

df_acima_media.show()

"""# 5: Filtragem - Média Truncada"""

df_media_truncada = df_resultado_final.filter(
    (col("valor_total") >= percentil_10) &
    (col("valor_total") <= percentil_90)
).orderBy(desc("valor_total"))

print("Clientes dentro da média truncada (P10–P90)")
df_media_truncada.show()

